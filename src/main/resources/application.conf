environment = "databricks"
environment = ${?SPARK_ENV}

spark {
  appName = "SparkScalaETLprojectforDatabricks_EMR"
  master = "local[*]"

  sql {
    shuffle.partitions = 200
    adaptive.enabled = true
    adaptive.coalescePartitions.enabled = true
    adaptive.skewJoin.enabled = true
  }

  databricks {
    delta.preview.enabled = true
  }
}

cloud {
  # S3 Configuration
  s3 {
    bucket = "my-data-bucket"
    bucket = ${?S3_BUCKET}
    region = "us-east-1"
    region = ${?AWS_REGION}
    usePathStyle = false
    usePathStyle = ${?S3_PATH_STYLE}
  }

  # DBFS Configuration (Databricks)
  dbfs {
    rootPath = "/dbfs/mnt/etl"
    rootPath = ${?DBFS_ROOT}
  }
}

source {
  # Source data paths
  csv {
    path = "data/input/csv"
    format = "csv"
    inferSchema = true
    header = true
    delimiter = ","
    nullValue = ""
    nanValue = "NaN"
  }

  json {
    path = "data/input/json"
    format = "json"
    multiLine = false
  }

  parquet {
    path = "data/input/parquet"
    format = "parquet"
  }
}

target {
  # Target database and table
  database = "etl_db"
  database = ${?TARGET_DB}

  warehouse {
    location = "s3://my-warehouse/tables"
    location = ${?WAREHOUSE_LOCATION}
  }

  # Write mode: overwrite or append
  writeMode = "overwrite"
  writeMode = ${?WRITE_MODE}

  # Use Delta Lake
  useDataFormat = "delta"
  useDataFormat = ${?DATA_FORMAT}

  # Table properties
  externalTable = true
  partitionBy = []
}

hive {
  # Metastore Configuration
  metastore {
    type = "glue"
    type = ${?METASTORE_TYPE}

    # For AWS Glue
    glue {
      catalogId = ""
      catalogId = ${?GLUE_CATALOG_ID}
    }

    # For Hive Metastore Service (HMS)
    hms {
      thriftUri = "thrift://localhost:9083"
      thriftUri = ${?HMS_THRIFT_URI}
      warehouse = "s3://hive-warehouse"
      warehouse = ${?HMS_WAREHOUSE}
    }
  }

  # Hive settings
  enabled = true
  partitionDiscovery = true
  validatePartitionColumns = true
}

transformations {
  # Data transformation settings
  trimColumns = ["name", "description", "email"]
  castSchema = true
  addLoadDate = true
  loadDateColumn = "load_dt"

  # Sample explicit casting rules
  schema {
    customer_id = "BIGINT"
    order_date = "DATE"
    amount = "DECIMAL(18,2)"
    customer_name = "STRING"
    is_active = "BOOLEAN"
  }
}

aws {
  # AWS EMR Configuration
  emr {
    clusterName = "spark-etl-cluster"
    clusterName = ${?EMR_CLUSTER_NAME}

    releaseLabel = "emr-7.1.0"
    releaseLabel = ${?EMR_RELEASE_VERSION}

    instanceType = "m5.xlarge"
    instanceType = ${?EMR_INSTANCE_TYPE}

    instanceCount = 3
    instanceCount = ${?EMR_INSTANCE_COUNT}

    logUri = "s3://my-emr-logs/spark-etl/"
    logUri = ${?EMR_LOG_URI}
  }

  # AWS credentials and roles
  credentials {
    assumeRoleArn = ""
    assumeRoleArn = ${?AWS_ASSUME_ROLE_ARN}

    sessionName = "spark-etl-session"
    sessionName = ${?AWS_ROLE_SESSION_NAME}
  }
}

job {
  # Job execution parameters
  retries = 1
  retries = ${?JOB_RETRIES}

  timeout = 3600
  timeout = ${?JOB_TIMEOUT}

  enableMetrics = true
  enableMetrics = ${?ENABLE_METRICS}

  enableCheckpoint = false
  enableCheckpoint = ${?ENABLE_CHECKPOINT}
}

logging {
  level = "INFO"
  level = ${?LOG_LEVEL}
}

